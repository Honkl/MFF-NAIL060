{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagtion implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-x)) # logsig\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)*(1.0-sigmoid(x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1.0 - x**2\n",
    "\n",
    "def lin(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, layers, activation='tanh'):\n",
    "        if activation == 'sigmoid':\n",
    "            self.activation = sigmoid\n",
    "            self.activation_prime = sigmoid_prime\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = tanh\n",
    "            self.activation_prime = tanh_prime\n",
    "\n",
    "        # Set weights\n",
    "        self.weights = []\n",
    "        # layers = [2,2,1]\n",
    "        # range of weight values (-1,1)\n",
    "        # input and hidden layers - random((2+1, 2+1)) : 3 x 3\n",
    "        for i in range(1, len(layers) - 1):\n",
    "            r = 2*np.random.random((layers[i-1] + 1, layers[i] + 1)) -1\n",
    "            self.weights.append(r)\n",
    "        # output layer - random((2+1, 1)) : 3 x 1\n",
    "        r = 2*np.random.random( (layers[i] + 1, layers[i+1])) - 1\n",
    "        self.weights.append(r)\n",
    "\n",
    "    # fits the network using backpropagation algorthm\n",
    "    def fit(self, X, y, learning_rate=0.2, epochs=100000):\n",
    "        # Add column of ones to X\n",
    "        # This is to add the bias unit to the input layer\n",
    "        ones = np.atleast_2d(np.ones(X.shape[0]))\n",
    "        X = np.concatenate((ones.T, X), axis=1)\n",
    "         \n",
    "        for k in range(epochs):\n",
    "            #if k % 10000 == 0: print('epochs:', k)\n",
    "            \n",
    "            i = np.random.randint(X.shape[0])\n",
    "            a = [X[i]]\n",
    "\n",
    "            for l in range(len(self.weights)):\n",
    "                    dot_value = np.dot(a[l], self.weights[l])\n",
    "                    activation = self.activation(dot_value)\n",
    "                    a.append(activation)\n",
    "            # output layer\n",
    "            error = y[i] - a[-1]\n",
    "            deltas = [error * self.activation_prime(a[-1])]\n",
    "\n",
    "            # we need to begin at the second to last layer \n",
    "            # (a layer before the output layer)\n",
    "            for l in range(len(a) - 2, 0, -1): \n",
    "                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_prime(a[l]))\n",
    "\n",
    "            # reverse\n",
    "            # [level3(output)->level2(hidden)]  => [level2(hidden)->level3(output)]\n",
    "            deltas.reverse()\n",
    "\n",
    "            # backpropagation\n",
    "            # 1. Multiply its output delta and input activation \n",
    "            #    to get the gradient of the weight.\n",
    "            # 2. Subtract a ratio (percentage) of the gradient from the weight.\n",
    "            for i in range(len(self.weights)):\n",
    "                layer = np.atleast_2d(a[i])\n",
    "                delta = np.atleast_2d(deltas[i])\n",
    "                self.weights[i] += learning_rate * layer.T.dot(delta)\n",
    "        \n",
    "        return self.weights\n",
    "\n",
    "    def predict(self, x): \n",
    "        a = np.concatenate((np.ones(1).T, np.array(x)), axis=0)      \n",
    "        for l in range(0, len(self.weights)):\n",
    "            a = self.activation(np.dot(a, self.weights[l]))\n",
    "        return a\n",
    "    \n",
    "#     def predict(self, x):\n",
    "#         x = np.array(x)\n",
    "#         temp = np.ones(x.shape[0]+1)\n",
    "#         temp[0:-1] = x\n",
    "#         a = temp\n",
    "#         for l in range(0, len(self.weights)):\n",
    "#             a = self.activation(np.dot(a, self.weights[l]))\n",
    "#         return a\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants + data generation\n",
    " Constants used in computations, read files with train + test data. Generation of the rest train data. Word transformations (\"word\" => bin)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some basic constants...\n",
    "LETTERS = 25\n",
    "MAX_WORD_LENGHT = 5\n",
    "CHAR_BITS = 5 # Enough for 25 letters\n",
    "#input_size = MAX_WORD_LENGHT * (len(bin(LETTERS))-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_word(word):\n",
    "    word = word.lower()\n",
    "    bin_word = ''.join([bin(ord(x)-ord('a'))[2:].zfill(CHAR_BITS) for x in word])\n",
    "    bin_word = bin_word + \"1\" * CHAR_BITS * (MAX_WORD_LENGHT - len(word))\n",
    "    return list(map(int, bin_word))\n",
    "\n",
    "def encode_unary(index, total_words):\n",
    "    zeros = [0] * total_words\n",
    "    zeros[index] = 1\n",
    "    return zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0], [1, 0, 0, 0], 'lorem'), ([0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0], [0, 1, 0, 0], 'ipsum'), ([0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 1, 0], 'es'), ([0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 1], 'et')]\n"
     ]
    }
   ],
   "source": [
    "f = \"table.txt\"\n",
    "with open (f, \"r\") as file:\n",
    "    raw_data = file.read().strip().split()\n",
    "\n",
    "data = []\n",
    "for word in raw_data:\n",
    "    if (len(word) <= MAX_WORD_LENGHT and word not in data):\n",
    "        data.append(word.lower())\n",
    "\n",
    "table = [(transform_word(data[index]), encode_unary(index, len(data)), data[index]) for index in range(len(data))]\n",
    "output_size = len(table)\n",
    "print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size = MAX_WORD_LENGHT * CHAR_BITS\n",
    "hidden_sizes = [10]\n",
    "layers = [input_size] + hidden_sizes + [output_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def modify_word_random(word_to_modif):\n",
    "    letter = np.random.choice(list(string.ascii_lowercase))\n",
    "    word = list(word_to_modif)\n",
    "    word[np.random.randint(len(word_to_modif))] = letter\n",
    "    return ''.join(word)\n",
    "    \n",
    "def gen_word(similar_prob):\n",
    "    if (np.random.uniform(0, 1) < similar_prob):\n",
    "        index = np.random.randint(len(table))\n",
    "        table_word = table[index][2]\n",
    "        return transform_word(modify_word_random(table_word))\n",
    "    else:\n",
    "        return [np.random.randint(2) for _ in range(input_size)]\n",
    "\n",
    "def generate_words(set_size, good_word_prob, similar_bad_word_prob=0.5):\n",
    "    X = np.array([table[0][0]])\n",
    "    y = np.array([table[0][1]])\n",
    "    for _ in range(set_size - 1):\n",
    "        if np.random.uniform(0, 1) < good_word_prob:\n",
    "            index = np.random.randint(len(table))\n",
    "            X = np.concatenate((X, [table[index][0]]))\n",
    "            y = np.concatenate((y, [table[index][1]]))\n",
    "        else:\n",
    "            found = False\n",
    "            while not found:\n",
    "                found = True\n",
    "                bad_word = gen_word(similar_bad_word_prob)\n",
    "                if (bad_word in [x for (x, y, z) in table]):\n",
    "                    found = False\n",
    "                    break\n",
    "                if found:\n",
    "                    X = np.concatenate((X, [bad_word]))\n",
    "                    y = np.concatenate((y, [[0] * output_size]))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x, train_y = generate_words(set_size=50000, good_word_prob=0.5, similar_bad_word_prob=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = \"data.txt\"\n",
    "with open (f, \"r\") as file:\n",
    "    raw_data = file.read().strip().split()\n",
    "\n",
    "escape_chars = [\".\", \",\", \"!\", \"?\", \";\"]\n",
    "    \n",
    "data = []\n",
    "for word in raw_data:\n",
    "    if (len(word) <= MAX_WORD_LENGHT):\n",
    "        w = word\n",
    "        for i in range(len(escape_chars)):\n",
    "            w = w.replace(escape_chars[i], \"\")\n",
    "\n",
    "        if (w.isalpha):\n",
    "            data.append(w.lower())\n",
    "        \n",
    "\n",
    "test_x = np.array([transform_word(w) for w in data])\n",
    "test_y = np.array([[0] * len(table) for _ in range(len(test_x))])\n",
    "test_z = np.array(data)\n",
    "\n",
    "for i in range(len(test_x)):\n",
    "    for j in range(len(table)):\n",
    "        if (np.array_equal(table[j][0], test_x[i])):\n",
    "            test_y[i] = table[j][1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning v1\n",
    "Using backprop \"hand\" implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit complete\n"
     ]
    }
   ],
   "source": [
    "net = NeuralNetwork(layers, activation='tanh')\n",
    "_ = net.fit(train_x, train_y, epochs=100000)\n",
    "print(\"Fit complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0] [1 0 0 0] [0.965, 0.01, 0.008, 0.016]\n",
      "[0 1 0 0 0 0 1 1 1 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0] [0 1 0 0] [0.008, 0.98, 0.009, 0.016]\n",
      "[0 0 0 1 1 0 1 1 1 0 0 1 0 1 1 0 1 1 1 0 1 0 0 0 1] [0 0 0 0] [0.013, 0.014, 0.005, 0.018]\n",
      "[1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1] [0 0 0 0] [0.014, 0.013, 0.006, 0.018]\n",
      "[0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 1 1 1] [0 0 0 0] [0.005, -0.004, 0.014, 0.015]\n",
      "[0 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 1] [0 0 0 0] [0.001, -0.001, 0.014, 0.015]\n",
      "[1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1] [0 0 0 0] [0.016, 0.015, 0.004, 0.017]\n",
      "[1 0 1 0 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1] [0 0 0 0] [0.026, 0.014, 0.0, 0.019]\n",
      "[0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 0] [0 0 0 0] [0.023, 0.009, 0.005, 0.028]\n",
      "[0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 0 1 1 1 0 1 1 1 1 1] [0 0 0 0] [0.01, 0.012, 0.004, 0.011]\n",
      "Succ:  0.989413409302\n",
      "Err:   0.0105865906985\n"
     ]
    }
   ],
   "source": [
    "def pretty_print(x, y, predicted_y):\n",
    "    print(x, y, [float(\"%.3f\" % value) for value in predicted_y])\n",
    "\n",
    "# success of the net using MSE\n",
    "def success(test_x, test_y):\n",
    "    MSE = 0\n",
    "    for (x, y) in zip(test_x, test_y):\n",
    "        prediction = net.predict(x)\n",
    "        MSE += sum((prediction - y) ** 2)\n",
    "    return 1 - (MSE / (len(test_x) * output_size))\n",
    "    \n",
    "show_max = 10\n",
    "count = 0\n",
    "for (x, y) in zip(test_x, test_y):\n",
    "    pretty_print(x, y, net.predict(x))\n",
    "    count += 1\n",
    "    if (show_max == count):\n",
    "        break\n",
    "    \n",
    "\n",
    "s = success(test_x, test_y)\n",
    "print(\"Succ: \", s)\n",
    "print(\"Err:  \", 1 - s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning v2\n",
    "Using scikit-learn MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.79251277\n",
      "Iteration 2, loss = 0.89536161\n",
      "Iteration 3, loss = 0.69400944\n",
      "Iteration 4, loss = 0.59013757\n",
      "Iteration 5, loss = 0.53145103\n",
      "Iteration 6, loss = 0.39282658\n",
      "Iteration 7, loss = 0.27680614\n",
      "Iteration 8, loss = 0.21839029\n",
      "Iteration 9, loss = 0.18183369\n",
      "Iteration 10, loss = 0.15518783\n",
      "Iteration 11, loss = 0.13506991\n",
      "Iteration 12, loss = 0.11967076\n",
      "Iteration 13, loss = 0.10729951\n",
      "Iteration 14, loss = 0.09690538\n",
      "Iteration 15, loss = 0.08821293\n",
      "Iteration 16, loss = 0.08079194\n",
      "Iteration 17, loss = 0.07433134\n",
      "Iteration 18, loss = 0.06871712\n",
      "Iteration 19, loss = 0.06360801\n",
      "Iteration 20, loss = 0.05921353\n",
      "Iteration 21, loss = 0.05513534\n",
      "Iteration 22, loss = 0.05151831\n",
      "Iteration 23, loss = 0.04803677\n",
      "Iteration 24, loss = 0.04497245\n",
      "Iteration 25, loss = 0.04217005\n",
      "Iteration 26, loss = 0.03940517\n",
      "Iteration 27, loss = 0.03693245\n",
      "Iteration 28, loss = 0.03473545\n",
      "Iteration 29, loss = 0.03259987\n",
      "Iteration 30, loss = 0.03066488\n",
      "Iteration 31, loss = 0.02890746\n",
      "Iteration 32, loss = 0.02719890\n",
      "Iteration 33, loss = 0.02559485\n",
      "Iteration 34, loss = 0.02412738\n",
      "Iteration 35, loss = 0.02278447\n",
      "Iteration 36, loss = 0.02155637\n",
      "Iteration 37, loss = 0.02026344\n",
      "Iteration 38, loss = 0.01915568\n",
      "Iteration 39, loss = 0.01807123\n",
      "Iteration 40, loss = 0.01712751\n",
      "Iteration 41, loss = 0.01614284\n",
      "Iteration 42, loss = 0.01528973\n",
      "Iteration 43, loss = 0.01447362\n",
      "Iteration 44, loss = 0.01365895\n",
      "Iteration 45, loss = 0.01294702\n",
      "Iteration 46, loss = 0.01231245\n",
      "Iteration 47, loss = 0.01163586\n",
      "Iteration 48, loss = 0.01106367\n",
      "Iteration 49, loss = 0.01049752\n",
      "Iteration 50, loss = 0.00996346\n",
      "Iteration 51, loss = 0.00947652\n",
      "Iteration 52, loss = 0.00906868\n",
      "Iteration 53, loss = 0.00853209\n",
      "Iteration 54, loss = 0.00808469\n",
      "Iteration 55, loss = 0.00768720\n",
      "Iteration 56, loss = 0.00731803\n",
      "Iteration 57, loss = 0.00695971\n",
      "Iteration 58, loss = 0.00652241\n",
      "Iteration 59, loss = 0.00617893\n",
      "Iteration 60, loss = 0.00582105\n",
      "Iteration 61, loss = 0.00544858\n",
      "Iteration 62, loss = 0.00511084\n",
      "Iteration 63, loss = 0.00478915\n",
      "Iteration 64, loss = 0.00451988\n",
      "Iteration 65, loss = 0.00426843\n",
      "Iteration 66, loss = 0.00393747\n",
      "Iteration 67, loss = 0.00370503\n",
      "Iteration 68, loss = 0.00346759\n",
      "Iteration 69, loss = 0.00324975\n",
      "Iteration 70, loss = 0.00303668\n",
      "Iteration 71, loss = 0.00285109\n",
      "Iteration 72, loss = 0.00266159\n",
      "Iteration 73, loss = 0.00246868\n",
      "Iteration 74, loss = 0.00233342\n",
      "Iteration 75, loss = 0.00215497\n",
      "Iteration 76, loss = 0.00200481\n",
      "Iteration 77, loss = 0.00187977\n",
      "Iteration 78, loss = 0.00176322\n",
      "Iteration 79, loss = 0.00164871\n",
      "Iteration 80, loss = 0.00152441\n",
      "Iteration 81, loss = 0.00141644\n",
      "Iteration 82, loss = 0.00133304\n",
      "Iteration 83, loss = 0.00124033\n",
      "Iteration 84, loss = 0.00114957\n",
      "Iteration 85, loss = 0.00107141\n",
      "Iteration 86, loss = 0.00099157\n",
      "Iteration 87, loss = 0.00092893\n",
      "Iteration 88, loss = 0.00085842\n",
      "Iteration 89, loss = 0.00079952\n",
      "Iteration 90, loss = 0.00073894\n",
      "Iteration 91, loss = 0.00068651\n",
      "Iteration 92, loss = 0.00063723\n",
      "Iteration 93, loss = 0.00058909\n",
      "Iteration 94, loss = 0.00055043\n",
      "Iteration 95, loss = 0.00050993\n",
      "Iteration 96, loss = 0.00047158\n",
      "Iteration 97, loss = 0.00043773\n",
      "Iteration 98, loss = 0.00040530\n",
      "Iteration 99, loss = 0.00037574\n",
      "Iteration 100, loss = 0.00035004\n",
      "Iteration 101, loss = 0.00033090\n",
      "Iteration 102, loss = 0.00030186\n",
      "Iteration 103, loss = 0.00028011\n",
      "Iteration 104, loss = 0.00026323\n",
      "Iteration 105, loss = 0.00024628\n",
      "Iteration 106, loss = 0.00023051\n",
      "Iteration 107, loss = 0.00021388\n",
      "Iteration 108, loss = 0.00019920\n",
      "Iteration 109, loss = 0.00018571\n",
      "Iteration 110, loss = 0.00017321\n",
      "Iteration 111, loss = 0.00016225\n",
      "Iteration 112, loss = 0.00015262\n",
      "Iteration 113, loss = 0.00014267\n",
      "Iteration 114, loss = 0.00013396\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "Loss function:  log_loss\n",
      "Score:  0.98040192659\n"
     ]
    }
   ],
   "source": [
    "from sklearn import neural_network as nn\n",
    "clf = nn.MLPClassifier(hidden_layer_sizes=(8, ),\n",
    "                       activation='relu',\n",
    "                       solver='adam',\n",
    "                       alpha=0.00001,\n",
    "                       verbose=True,\n",
    "                       tol=0.00001,\n",
    "                       max_iter=300,\n",
    "                       learning_rate='constant',\n",
    "                       learning_rate_init=0.001)\n",
    "\n",
    "model = clf.fit(train_x, train_y)\n",
    "predictions = model.predict_proba(test_x)\n",
    "\n",
    "print(\"Loss function: \", model.loss)\n",
    "#print(\"Loss value: \", model.loss_)\n",
    "print(\"Score: \", model.score(test_x, test_y)) # score = mean accuracy on given data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tolerance = 0.2\n",
    "\n",
    "def round_prediction(y):\n",
    "    result = []\n",
    "    for i in range(len(y)):\n",
    "        if (y[i] - tolerance <= 0):\n",
    "            result.append(0)\n",
    "        elif (y[i] + tolerance >= 1):\n",
    "            result.append(1)\n",
    "        else:\n",
    "            return np.zeros(len(y))\n",
    "    return result\n",
    "    \n",
    "rounded_predictions = np.copy(predictions)\n",
    "\n",
    "# Round predictions based on tolerance:\n",
    "for i in range(len(predictions)):\n",
    "    rounded_predictions[i] = round_prediction(predictions[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test for table (positive words): \n",
      "Score on table words:  1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Test for table (positive words): \")\n",
    "\n",
    "\n",
    "table_test_x = []\n",
    "table_test_y = []\n",
    "for i in range(len(table)):\n",
    "    table_test_x.append(table[i][0])\n",
    "    table_test_y.append(table[i][1])\n",
    "    \n",
    "table_test_x = np.array(table_test_x)\n",
    "table_test_y = np.array(table_test_y)\n",
    "\n",
    "\n",
    "print(\"Score on table words: \", model.score(table_test_x, table_test_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " WORD  ->  PREDICTED (first 20 words)\n",
      "=====================================\n",
      "  est  ->  es    [0 0 0 0]  ->  [0.0, 0.0, 0.967, 0.003]\n",
      "  est  ->  es    [0 0 0 0]  ->  [0.0, 0.0, 0.967, 0.003]\n",
      "  est  ->  es    [0 0 0 0]  ->  [0.0, 0.0, 0.967, 0.003]\n",
      "  est  ->  es    [0 0 0 0]  ->  [0.0, 0.0, 0.967, 0.003]\n",
      "error  ->  et    [0 0 0 0]  ->  [0.0, 0.0, 0.0, 0.947]\n",
      "  est  ->  es    [0 0 0 0]  ->  [0.0, 0.0, 0.967, 0.003]\n",
      "  est  ->  es    [0 0 0 0]  ->  [0.0, 0.0, 0.967, 0.003]\n",
      "  est  ->  es    [0 0 0 0]  ->  [0.0, 0.0, 0.967, 0.003]\n",
      "  est  ->  es    [0 0 0 0]  ->  [0.0, 0.0, 0.967, 0.003]\n",
      "  est  ->  es    [0 0 0 0]  ->  [0.0, 0.0, 0.967, 0.003]\n",
      "error  ->  et    [0 0 0 0]  ->  [0.0, 0.0, 0.0, 0.947]\n",
      "error  ->  et    [0 0 0 0]  ->  [0.0, 0.0, 0.0, 0.947]\n",
      "  est  ->  es    [0 0 0 0]  ->  [0.0, 0.0, 0.967, 0.003]\n",
      "  est  ->  es    [0 0 0 0]  ->  [0.0, 0.0, 0.967, 0.003]\n",
      "  est  ->  es    [0 0 0 0]  ->  [0.0, 0.0, 0.967, 0.003]\n",
      "  est  ->  es    [0 0 0 0]  ->  [0.0, 0.0, 0.967, 0.003]\n",
      "  est  ->  es    [0 0 0 0]  ->  [0.0, 0.0, 0.967, 0.003]\n",
      "  est  ->  es    [0 0 0 0]  ->  [0.0, 0.0, 0.967, 0.003]\n",
      "  est  ->  es    [0 0 0 0]  ->  [0.0, 0.0, 0.967, 0.003]\n",
      "  est  ->  es    [0 0 0 0]  ->  [0.0, 0.0, 0.967, 0.003]\n"
     ]
    }
   ],
   "source": [
    "print(\" WORD  ->  PREDICTED (first 20 words)\")\n",
    "print(\"=====================================\")\n",
    "\n",
    "mistakes = []\n",
    "\n",
    "printed = 0\n",
    "for i in range(len(test_x)):\n",
    "    if not np.array_equal(rounded_predictions[i], test_y[i]):\n",
    "        for j in range(len(table)):\n",
    "            predicted_word = \"\"\n",
    "            if (np.array_equal(table[j][1], rounded_predictions[i])):\n",
    "                predicted_word = table[j][2]\n",
    "                break\n",
    "        #print(test_y[i], predictions[i])\n",
    "        print(test_z[i].rjust(MAX_WORD_LENGHT), \" -> \", predicted_word, \"  \", test_y[i], \" -> \", [float(\"%.3f\" % value) for value in predictions[i]])\n",
    "        printed += 1\n",
    "    if (printed >= 20):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEACAYAAAC9Gb03AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/X+NS7IJ7d/rcQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2201de0e320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(model.loss_curve_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
