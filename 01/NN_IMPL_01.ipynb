{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagtion implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-x)) # logsig\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)*(1.0-sigmoid(x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1.0 - x**2\n",
    "\n",
    "def lin(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, layers, activation='tanh'):\n",
    "        if activation == 'sigmoid':\n",
    "            self.activation = sigmoid\n",
    "            self.activation_prime = sigmoid_prime\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = tanh\n",
    "            self.activation_prime = tanh_prime\n",
    "\n",
    "        # Set weights\n",
    "        self.weights = []\n",
    "        # layers = [2,2,1]\n",
    "        # range of weight values (-1,1)\n",
    "        # input and hidden layers - random((2+1, 2+1)) : 3 x 3\n",
    "        for i in range(1, len(layers) - 1):\n",
    "            r = 2*np.random.random((layers[i-1] + 1, layers[i] + 1)) -1\n",
    "            self.weights.append(r)\n",
    "        # output layer - random((2+1, 1)) : 3 x 1\n",
    "        r = 2*np.random.random( (layers[i] + 1, layers[i+1])) - 1\n",
    "        self.weights.append(r)\n",
    "\n",
    "    # fits the network using backpropagation algorthm\n",
    "    def fit(self, X, y, learning_rate=0.2, epochs=100000):\n",
    "        # Add column of ones to X\n",
    "        # This is to add the bias unit to the input layer\n",
    "        ones = np.atleast_2d(np.ones(X.shape[0]))\n",
    "        X = np.concatenate((ones.T, X), axis=1)\n",
    "         \n",
    "        for k in range(epochs):\n",
    "            #if k % 10000 == 0: print('epochs:', k)\n",
    "            \n",
    "            i = np.random.randint(X.shape[0])\n",
    "            a = [X[i]]\n",
    "\n",
    "            for l in range(len(self.weights)):\n",
    "                    dot_value = np.dot(a[l], self.weights[l])\n",
    "                    activation = self.activation(dot_value)\n",
    "                    a.append(activation)\n",
    "            # output layer\n",
    "            error = y[i] - a[-1]\n",
    "            deltas = [error * self.activation_prime(a[-1])]\n",
    "\n",
    "            # we need to begin at the second to last layer \n",
    "            # (a layer before the output layer)\n",
    "            for l in range(len(a) - 2, 0, -1): \n",
    "                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_prime(a[l]))\n",
    "\n",
    "            # reverse\n",
    "            # [level3(output)->level2(hidden)]  => [level2(hidden)->level3(output)]\n",
    "            deltas.reverse()\n",
    "\n",
    "            # backpropagation\n",
    "            # 1. Multiply its output delta and input activation \n",
    "            #    to get the gradient of the weight.\n",
    "            # 2. Subtract a ratio (percentage) of the gradient from the weight.\n",
    "            for i in range(len(self.weights)):\n",
    "                layer = np.atleast_2d(a[i])\n",
    "                delta = np.atleast_2d(deltas[i])\n",
    "                self.weights[i] += learning_rate * layer.T.dot(delta)\n",
    "        \n",
    "        return self.weights\n",
    "\n",
    "    def predict(self, x): \n",
    "        a = np.concatenate((np.ones(1).T, np.array(x)), axis=0)      \n",
    "        for l in range(0, len(self.weights)):\n",
    "            a = self.activation(np.dot(a, self.weights[l]))\n",
    "        return a\n",
    "    \n",
    "#     def predict(self, x):\n",
    "#         x = np.array(x)\n",
    "#         temp = np.ones(x.shape[0]+1)\n",
    "#         temp[0:-1] = x\n",
    "#         a = temp\n",
    "#         for l in range(0, len(self.weights)):\n",
    "#             a = self.activation(np.dot(a, self.weights[l]))\n",
    "#         return a\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants + data generation\n",
    " Constants used in computations, read files with train + test data. Generation of the rest train data. Word transformations (\"word\" => bin)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some basic constants...\n",
    "LETTERS = 25\n",
    "MAX_WORD_LENGHT = 5\n",
    "CHAR_BITS = 5 # Enough for 25 letters\n",
    "#input_size = MAX_WORD_LENGHT * (len(bin(LETTERS))-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_word(word):\n",
    "    word = word.lower()\n",
    "    bin_word = ''.join([bin(ord(x)-ord('a'))[2:].zfill(CHAR_BITS) for x in word])\n",
    "    bin_word = bin_word + \"1\" * CHAR_BITS * (MAX_WORD_LENGHT - len(word))\n",
    "    return list(map(int, bin_word))\n",
    "\n",
    "def encode_unary(index, total_words):\n",
    "    zeros = [0] * total_words\n",
    "    zeros[index] = 1\n",
    "    return zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0], [1, 0, 0, 0], 'lorem'), ([0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0], [0, 1, 0, 0], 'ipsum'), ([0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 1, 0], 'et'), ([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0], [0, 0, 0, 1], 'nulla')]\n"
     ]
    }
   ],
   "source": [
    "f = \"table.txt\"\n",
    "with open (f, \"r\") as file:\n",
    "    raw_data = file.read().strip().split()\n",
    "\n",
    "data = []\n",
    "for word in raw_data:\n",
    "    if (len(word) <= MAX_WORD_LENGHT and word not in data):\n",
    "        data.append(word.lower())\n",
    "\n",
    "table = [(transform_word(data[index]), encode_unary(index, len(data)), data[index]) for index in range(len(data))]\n",
    "output_size = len(table)\n",
    "print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size = MAX_WORD_LENGHT * CHAR_BITS\n",
    "hidden_sizes = [10]\n",
    "layers = [input_size] + hidden_sizes + [output_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def modify_word_random(word_to_modif):\n",
    "    letter = np.random.choice(list(string.ascii_lowercase))\n",
    "    word = list(word_to_modif)\n",
    "    word[np.random.randint(len(word_to_modif))] = letter\n",
    "    return ''.join(word)\n",
    "    \n",
    "def gen_word(similar_prob):\n",
    "    if (np.random.uniform(0, 1) < similar_prob):\n",
    "        index = np.random.randint(len(table))\n",
    "        table_word = table[index][2]\n",
    "        return transform_word(modify_word_random(table_word))\n",
    "    else:\n",
    "        return [np.random.randint(2) for _ in range(input_size)]\n",
    "\n",
    "def generate_words(set_size, good_word_prob, similar_bad_word_prob=0.5):\n",
    "    X = np.array([table[0][0]])\n",
    "    y = np.array([table[0][1]])\n",
    "    for _ in range(set_size - 1):\n",
    "        if np.random.uniform(0, 1) < good_word_prob:\n",
    "            index = np.random.randint(len(table))\n",
    "            X = np.concatenate((X, [table[index][0]]))\n",
    "            y = np.concatenate((y, [table[index][1]]))\n",
    "        else:\n",
    "            found = False\n",
    "            while not found:\n",
    "                found = True\n",
    "                bad_word = gen_word(similar_bad_word_prob)\n",
    "                if (bad_word in [x for (x, y, z) in table]):\n",
    "                    found = False\n",
    "                    break\n",
    "                if found:\n",
    "                    X = np.concatenate((X, [bad_word]))\n",
    "                    y = np.concatenate((y, [[0] * output_size]))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x, train_y = generate_words(set_size=100000, good_word_prob=0.5, similar_bad_word_prob=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = \"data.txt\"\n",
    "with open (f, \"r\") as file:\n",
    "    raw_data = file.read().strip().split()\n",
    "\n",
    "escape_chars = [\".\", \",\", \"!\", \"?\", \";\"]\n",
    "    \n",
    "data = []\n",
    "for word in raw_data:\n",
    "    if (len(word) <= MAX_WORD_LENGHT):\n",
    "        w = word\n",
    "        for i in range(len(escape_chars)):\n",
    "            w = w.replace(escape_chars[i], \"\")\n",
    "\n",
    "        if (w.isalpha):\n",
    "            data.append(w.lower())\n",
    "        \n",
    "\n",
    "test_x = np.array([transform_word(w) for w in data])\n",
    "test_y = np.array([[0] * len(table) for _ in range(len(test_x))])\n",
    "test_z = np.array(data)\n",
    "\n",
    "for i in range(len(test_x)):\n",
    "    for j in range(len(table)):\n",
    "        if (np.array_equal(table[j][0], test_x[i])):\n",
    "            test_y[i] = table[j][1]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning v1\n",
    "Using backprop \"hand\" implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit complete\n"
     ]
    }
   ],
   "source": [
    "net = NeuralNetwork(layers, activation='tanh')\n",
    "_ = net.fit(train_x, train_y, epochs=100000)\n",
    "print(\"Fit complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " word       --     word binary representation    --    desired output   --   real output\n",
      "========================================================================================\n",
      "lorem [0 1 0 1 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0] [1 0 0 0] [0.985, -0.01, 0.017, 0.069]\n",
      "ipsum [0 1 0 0 0 0 1 1 1 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0] [0 1 0 0] [-0.017, 0.972, -0.005, -0.032]\n",
      "dolor [0 0 0 1 1 0 1 1 1 0 0 1 0 1 1 0 1 1 1 0 1 0 0 0 1] [0 0 0 0] [-0.015, -0.036, -0.014, -0.202]\n",
      "  sit [1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1] [0 0 0 0] [-0.018, -0.033, -0.011, -0.18]\n",
      " amet [0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 1 1 1] [0 0 0 0] [-0.03, -0.021, -0.006, -0.067]\n",
      " elit [0 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 1] [0 0 0 0] [-0.005, -0.027, 0.001, -0.079]\n",
      "  sed [1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1] [0 0 0 0] [-0.013, -0.035, -0.011, -0.208]\n",
      "  vel [1 0 1 0 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1] [0 0 0 0] [-0.015, -0.028, -0.013, -0.177]\n",
      "donec [0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 0] [0 0 0 0] [-0.015, -0.034, -0.012, -0.203]\n",
      " odio [0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 0 1 1 1 0 1 1 1 1 1] [0 0 0 0] [-0.009, -0.016, -0.003, -0.029]\n",
      "Succ:  0.989543491102\n",
      "Err:   0.0104565088981\n"
     ]
    }
   ],
   "source": [
    "def pretty_print(x, y, predicted_y, z):\n",
    "    print(z.rjust(MAX_WORD_LENGHT), x, y, [float(\"%.3f\" % value) for value in predicted_y])\n",
    "\n",
    "# success of the net using MSE\n",
    "def success(test_x, test_y):\n",
    "    MSE = 0\n",
    "    for (x, y) in zip(test_x, test_y):\n",
    "        prediction = net.predict(x)\n",
    "        MSE += sum((prediction - y) ** 2)\n",
    "    return 1 - (MSE / (len(test_x) * output_size))\n",
    "    \n",
    "    \n",
    "print(\" word       --     word binary representation    --    desired output   --   real output\")\n",
    "print(\"========================================================================================\")\n",
    "show_max = 10\n",
    "count = 0\n",
    "for (x, y, z) in zip(test_x, test_y, test_z):\n",
    "    pretty_print(x, y, net.predict(x), z)\n",
    "    count += 1\n",
    "    if (show_max == count):\n",
    "        break\n",
    "    \n",
    "\n",
    "s = success(test_x, test_y)\n",
    "print(\"Succ: \", s)\n",
    "print(\"Err:  \", 1 - s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning v2\n",
    "Using scikit-learn MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.07461584\n",
      "Iteration 2, loss = 0.35312063\n",
      "Iteration 3, loss = 0.24742705\n",
      "Iteration 4, loss = 0.18880512\n",
      "Iteration 5, loss = 0.14862456\n",
      "Iteration 6, loss = 0.11737841\n",
      "Iteration 7, loss = 0.09030163\n",
      "Iteration 8, loss = 0.07112317\n",
      "Iteration 9, loss = 0.05584651\n",
      "Iteration 10, loss = 0.04391191\n",
      "Iteration 11, loss = 0.03411313\n",
      "Iteration 12, loss = 0.02615406\n",
      "Iteration 13, loss = 0.02022914\n",
      "Iteration 14, loss = 0.01568133\n",
      "Iteration 15, loss = 0.01212357\n",
      "Iteration 16, loss = 0.00940791\n",
      "Iteration 17, loss = 0.00732008\n",
      "Iteration 18, loss = 0.00570975\n",
      "Iteration 19, loss = 0.00446297\n",
      "Iteration 20, loss = 0.00351174\n",
      "Iteration 21, loss = 0.00276930\n",
      "Iteration 22, loss = 0.00220030\n",
      "Iteration 23, loss = 0.00176023\n",
      "Iteration 24, loss = 0.00142734\n",
      "Iteration 25, loss = 0.00115926\n",
      "Iteration 26, loss = 0.00095286\n",
      "Iteration 27, loss = 0.00079496\n",
      "Iteration 28, loss = 0.00067369\n",
      "Iteration 29, loss = 0.00057656\n",
      "Iteration 30, loss = 0.00050205\n",
      "Iteration 31, loss = 0.00044300\n",
      "Iteration 32, loss = 0.00039111\n",
      "Iteration 33, loss = 0.00035653\n",
      "Iteration 34, loss = 0.00032821\n",
      "Iteration 35, loss = 0.00030750\n",
      "Iteration 36, loss = 0.00029145\n",
      "Iteration 37, loss = 0.00027856\n",
      "Iteration 38, loss = 0.00026839\n",
      "Iteration 39, loss = 0.00026103\n",
      "Iteration 40, loss = 0.00025561\n",
      "Iteration 41, loss = 0.00025119\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n",
      "Loss function:  log_loss\n",
      "Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import neural_network as nn\n",
    "clf = nn.MLPClassifier(hidden_layer_sizes=(16, ),\n",
    "                       activation='relu',\n",
    "                       solver='adam',\n",
    "                       alpha=0.0001,\n",
    "                       verbose=True,\n",
    "                       tol=0.00001,\n",
    "                       max_iter=300,\n",
    "                       learning_rate='constant',\n",
    "                       learning_rate_init=0.001)\n",
    "\n",
    "model = clf.fit(train_x, train_y)\n",
    "predictions = model.predict_proba(test_x)\n",
    "\n",
    "print(\"Loss function: \", model.loss)\n",
    "#print(\"Loss value: \", model.loss_)\n",
    "print(\"Score: \", model.score(test_x, test_y)) # score = mean accuracy on given data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tolerance = 0.2\n",
    "\n",
    "def round_prediction(y):\n",
    "    result = []\n",
    "    for i in range(len(y)):\n",
    "        if (y[i] - tolerance <= 0):\n",
    "            result.append(0)\n",
    "        elif (y[i] + tolerance >= 1):\n",
    "            result.append(1)\n",
    "        else:\n",
    "            return np.zeros(len(y))\n",
    "    return result\n",
    "    \n",
    "rounded_predictions = np.copy(predictions)\n",
    "\n",
    "# Round predictions based on tolerance:\n",
    "for i in range(len(predictions)):\n",
    "    rounded_predictions[i] = round_prediction(predictions[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test for table (positive words): \n",
      "Score on table words:  1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Test for table (positive words): \")\n",
    "\n",
    "\n",
    "table_test_x = []\n",
    "table_test_y = []\n",
    "for i in range(len(table)):\n",
    "    table_test_x.append(table[i][0])\n",
    "    table_test_y.append(table[i][1])\n",
    "    \n",
    "table_test_x = np.array(table_test_x)\n",
    "table_test_y = np.array(table_test_y)\n",
    "\n",
    "print(\"Score on table words: \", model.score(table_test_x, table_test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " WORD  ->  PREDICTED\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "print(\" WORD  ->  PREDICTED\")\n",
    "print(\"====================\")\n",
    "\n",
    "mistakes = []\n",
    "\n",
    "printed = 0\n",
    "for i in range(len(test_x)):\n",
    "    if not np.array_equal(rounded_predictions[i], test_y[i]):\n",
    "        for j in range(len(table)):\n",
    "            predicted_word = \"\"\n",
    "            if (np.array_equal(table[j][1], rounded_predictions[i])):\n",
    "                predicted_word = table[j][2]\n",
    "                break\n",
    "        #print(test_y[i], predictions[i])\n",
    "        mistakes.append([test_z[i].rjust(MAX_WORD_LENGHT),\n",
    "                         \" -> \",\n",
    "                         predicted_word,\n",
    "                         \"  \",\n",
    "                         test_y[i],\n",
    "                         \" -> \",\n",
    "                         [float(\"%.3f\" % value) for value in predictions[i]]])\n",
    "\n",
    "unique = []\n",
    "for line in mistakes:\n",
    "    line_str = ''.join([str(x) for x in line])\n",
    "    if not line_str in unique:\n",
    "        unique.append(line_str)\n",
    "        print(line_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEKCAYAAADpfBXhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25864b07400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(model.loss_curve_)\n",
    "plt.title(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
