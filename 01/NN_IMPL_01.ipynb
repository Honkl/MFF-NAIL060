{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-x)) # logsig\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)*(1.0-sigmoid(x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1.0 - x**2\n",
    "\n",
    "def lin(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, layers, activation='tanh'):\n",
    "        if activation == 'sigmoid':\n",
    "            self.activation = sigmoid\n",
    "            self.activation_prime = sigmoid_prime\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = tanh\n",
    "            self.activation_prime = tanh_prime\n",
    "\n",
    "        # Set weights\n",
    "        self.weights = []\n",
    "        # layers = [2,2,1]\n",
    "        # range of weight values (-1,1)\n",
    "        # input and hidden layers - random((2+1, 2+1)) : 3 x 3\n",
    "        for i in range(1, len(layers) - 1):\n",
    "            r = 2*np.random.random((layers[i-1] + 1, layers[i] + 1)) -1\n",
    "            self.weights.append(r)\n",
    "        # output layer - random((2+1, 1)) : 3 x 1\n",
    "        r = 2*np.random.random( (layers[i] + 1, layers[i+1])) - 1\n",
    "        self.weights.append(r)\n",
    "\n",
    "    # fits the network using backpropagation algorthm\n",
    "    def fit(self, X, y, learning_rate=0.2, epochs=100000):\n",
    "        # Add column of ones to X\n",
    "        # This is to add the bias unit to the input layer\n",
    "        ones = np.atleast_2d(np.ones(X.shape[0]))\n",
    "        X = np.concatenate((ones.T, X), axis=1)\n",
    "         \n",
    "        for k in range(epochs):\n",
    "            #if k % 10000 == 0: print('epochs:', k)\n",
    "            \n",
    "            i = np.random.randint(X.shape[0])\n",
    "            a = [X[i]]\n",
    "\n",
    "            for l in range(len(self.weights)):\n",
    "                    dot_value = np.dot(a[l], self.weights[l])\n",
    "                    activation = self.activation(dot_value)\n",
    "                    a.append(activation)\n",
    "            # output layer\n",
    "            error = y[i] - a[-1]\n",
    "            deltas = [error * self.activation_prime(a[-1])]\n",
    "\n",
    "            # we need to begin at the second to last layer \n",
    "            # (a layer before the output layer)\n",
    "            for l in range(len(a) - 2, 0, -1): \n",
    "                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_prime(a[l]))\n",
    "\n",
    "            # reverse\n",
    "            # [level3(output)->level2(hidden)]  => [level2(hidden)->level3(output)]\n",
    "            deltas.reverse()\n",
    "\n",
    "            # backpropagation\n",
    "            # 1. Multiply its output delta and input activation \n",
    "            #    to get the gradient of the weight.\n",
    "            # 2. Subtract a ratio (percentage) of the gradient from the weight.\n",
    "            for i in range(len(self.weights)):\n",
    "                layer = np.atleast_2d(a[i])\n",
    "                delta = np.atleast_2d(deltas[i])\n",
    "                self.weights[i] += learning_rate * layer.T.dot(delta)\n",
    "        \n",
    "        return self.weights\n",
    "\n",
    "    def predict(self, x): \n",
    "        a = np.concatenate((np.ones(1).T, np.array(x)), axis=0)      \n",
    "        for l in range(0, len(self.weights)):\n",
    "            a = self.activation(np.dot(a, self.weights[l]))\n",
    "        return a\n",
    "    \n",
    "#     def predict(self, x):\n",
    "#         x = np.array(x)\n",
    "#         temp = np.ones(x.shape[0]+1)\n",
    "#         temp[0:-1] = x\n",
    "#         a = temp\n",
    "#         for l in range(0, len(self.weights)):\n",
    "#             a = self.activation(np.dot(a, self.weights[l]))\n",
    "#         return a\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some basic constants...\n",
    "LETTERS = 25\n",
    "MAX_WORD_LENGHT = 5\n",
    "CHAR_BITS = 5 # Enough for 25 letters\n",
    "#input_size = MAX_WORD_LENGHT * (len(bin(LETTERS))-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_word(word):\n",
    "    word = word.lower()\n",
    "    bin_word = ''.join([bin(ord(x)-ord('a'))[2:].zfill(CHAR_BITS) for x in word])\n",
    "    bin_word = bin_word + \"1\" * CHAR_BITS * (MAX_WORD_LENGHT - len(word))\n",
    "    return list(map(int, bin_word))\n",
    "\n",
    "def encode_unary(index, total_words):\n",
    "    zeros = [0] * total_words\n",
    "    zeros[index] = 1\n",
    "    return zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0], [1, 0, 0, 0], 'lorem'), ([0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0], [0, 1, 0, 0], 'ipsum'), ([0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 1, 0], 'es'), ([0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 1], 'et')]\n"
     ]
    }
   ],
   "source": [
    "f = \"table.txt\"\n",
    "with open (f, \"r\") as file:\n",
    "    raw_data = file.read().strip().split()\n",
    "\n",
    "data = []\n",
    "for word in raw_data:\n",
    "    if (len(word) <= MAX_WORD_LENGHT and word not in data):\n",
    "        data.append(word.lower())\n",
    "\n",
    "table = [(transform_word(data[index]), encode_unary(index, len(data)), data[index]) for index in range(len(data))]\n",
    "output_size = len(table)\n",
    "print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size = MAX_WORD_LENGHT * CHAR_BITS\n",
    "hidden_sizes = [10]\n",
    "layers = [input_size] + hidden_sizes + [output_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_words(set_size, good_word_prob):\n",
    "    X = np.array([table[0][0]])\n",
    "    y = np.array([table[0][1]])\n",
    "    for _ in range(set_size - 1):\n",
    "        if np.random.uniform(0, 1) < good_word_prob:\n",
    "            index = np.random.randint(len(table))\n",
    "            X = np.concatenate((X, [table[index][0]]))\n",
    "            y = np.concatenate((y, [table[index][1]]))\n",
    "        else:\n",
    "            found = False\n",
    "            while not found:\n",
    "                found = True\n",
    "                bad_word = [np.random.randint(2) for _ in range(input_size)]\n",
    "                for i in range(len(table)):\n",
    "                    if (table[i][0] == bad_word):\n",
    "                        found = False\n",
    "                        break\n",
    "                if found:\n",
    "                    X = np.concatenate((X, [bad_word]))\n",
    "                    y = np.concatenate((y, [[0] * output_size]))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x, train_y = generate_words(set_size=20000, good_word_prob=0.6)\n",
    "#test_x, test_y = generate_words(set_size=1000, good_word_prob=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = \"data.txt\"\n",
    "with open (f, \"r\") as file:\n",
    "    raw_data = file.read().strip().split()\n",
    "\n",
    "escape_chars = [\".\", \",\", \"!\", \"?\", \";\"]\n",
    "    \n",
    "data = []\n",
    "for word in raw_data:\n",
    "    if (len(word) <= MAX_WORD_LENGHT):\n",
    "        w = word\n",
    "        for i in range(len(escape_chars)):\n",
    "            w = w.replace(escape_chars[i], \"\")\n",
    "\n",
    "        if (w.isalpha):\n",
    "            data.append(w.lower())\n",
    "        \n",
    "\n",
    "test_x = np.array([transform_word(w) for w in data])\n",
    "test_y = np.array([[0] * len(table) for _ in range(len(test_x))])\n",
    "test_z = np.array(data)\n",
    "\n",
    "for i in range(len(test_x)):\n",
    "    for j in range(len(table)):\n",
    "        if (np.array_equal(table[j][0], test_x[i])):\n",
    "            test_y[i] = table[j][1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit complete\n"
     ]
    }
   ],
   "source": [
    "net = NeuralNetwork(layers, activation='tanh')\n",
    "_ = net.fit(train_x, train_y, epochs=100000)\n",
    "print(\"Fit complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0] [1 0 0 0] [0.994, 0.002, 0.001, 0.001]\n",
      "[0 1 0 0 0 0 1 1 1 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0] [0 1 0 0] [-0.009, 0.993, -0.004, -0.006]\n",
      "[0 0 0 1 1 0 1 1 1 0 0 1 0 1 1 0 1 1 1 0 1 0 0 0 1] [0 0 0 0] [-0.003, 0.008, 0.005, 0.007]\n",
      "[1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1] [0 0 0 0] [-0.005, 0.004, 0.003, 0.003]\n",
      "[0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 1 1 1] [0 0 0 0] [-0.007, -0.003, -0.006, -0.002]\n",
      "[0 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 1] [0 0 0 0] [-0.007, -0.002, -0.003, 0.012]\n",
      "[1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1] [0 0 0 0] [-0.003, 0.009, 0.005, 0.005]\n",
      "[1 0 1 0 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1] [0 0 0 0] [-0.003, 0.008, 0.004, 0.006]\n",
      "[0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 0] [0 0 0 0] [-0.001, 0.009, 0.004, 0.006]\n",
      "[0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 0 1 1 1 0 1 1 1 1 1] [0 0 0 0] [-0.006, 0.001, -0.001, 0.001]\n",
      "Succ:  0.983514950313\n",
      "Err:   0.0164850496873\n"
     ]
    }
   ],
   "source": [
    "def pretty_print(x, y, predicted_y):\n",
    "    print(x, y, [float(\"%.3f\" % value) for value in predicted_y])\n",
    "\n",
    "# success of the net using MSE\n",
    "def success(test_x, test_y):\n",
    "    MSE = 0\n",
    "    for (x, y) in zip(test_x, test_y):\n",
    "        prediction = net.predict(x)\n",
    "        MSE += sum((prediction - y) ** 2)\n",
    "    return 1 - (MSE / (len(test_x) * output_size))\n",
    "    \n",
    "show_max = 10\n",
    "count = 0\n",
    "for (x, y) in zip(test_x, test_y):\n",
    "    pretty_print(x, y, net.predict(x))\n",
    "    count += 1\n",
    "    if (show_max == count):\n",
    "        break\n",
    "    \n",
    "\n",
    "s = success(test_x, test_y)\n",
    "print(\"Succ: \", s)\n",
    "print(\"Err:  \", 1 - s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.10851759\n",
      "Iteration 2, loss = 1.25250804\n",
      "Iteration 3, loss = 0.90546120\n",
      "Iteration 4, loss = 0.69440444\n",
      "Iteration 5, loss = 0.55545223\n",
      "Iteration 6, loss = 0.45174019\n",
      "Iteration 7, loss = 0.37099704\n",
      "Iteration 8, loss = 0.30632446\n",
      "Iteration 9, loss = 0.25423577\n",
      "Iteration 10, loss = 0.21274537\n",
      "Iteration 11, loss = 0.17966537\n",
      "Iteration 12, loss = 0.15338897\n",
      "Iteration 13, loss = 0.13227074\n",
      "Iteration 14, loss = 0.11513214\n",
      "Iteration 15, loss = 0.10108758\n",
      "Iteration 16, loss = 0.08942400\n",
      "Iteration 17, loss = 0.07965300\n",
      "Iteration 18, loss = 0.07144361\n",
      "Iteration 19, loss = 0.06444960\n",
      "Iteration 20, loss = 0.05837427\n",
      "Iteration 21, loss = 0.05314652\n",
      "Iteration 22, loss = 0.04860173\n",
      "Iteration 23, loss = 0.04462304\n",
      "Iteration 24, loss = 0.04108157\n",
      "Iteration 25, loss = 0.03796897\n",
      "Iteration 26, loss = 0.03516881\n",
      "Iteration 27, loss = 0.03266050\n",
      "Iteration 28, loss = 0.03041298\n",
      "Iteration 29, loss = 0.02835693\n",
      "Iteration 30, loss = 0.02652368\n",
      "Iteration 31, loss = 0.02481730\n",
      "Iteration 32, loss = 0.02325458\n",
      "Iteration 33, loss = 0.02188154\n",
      "Iteration 34, loss = 0.02056254\n",
      "Iteration 35, loss = 0.01936212\n",
      "Iteration 36, loss = 0.01821176\n",
      "Iteration 37, loss = 0.01717641\n",
      "Iteration 38, loss = 0.01623674\n",
      "Iteration 39, loss = 0.01533601\n",
      "Iteration 40, loss = 0.01450058\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Loss function:  log_loss\n",
      "Score:  0.905829596413\n"
     ]
    }
   ],
   "source": [
    "from sklearn import neural_network as nn\n",
    "clf = nn.MLPClassifier(hidden_layer_sizes=(16, ),\n",
    "                       activation='tanh',\n",
    "                       solver='adam',\n",
    "                       alpha=0.00001,\n",
    "                       verbose=True,\n",
    "                       tol=0.001,\n",
    "                       learning_rate='constant',\n",
    "                       learning_rate_init=0.001)\n",
    "\n",
    "model = clf.fit(train_x, train_y)\n",
    "predictions = model.predict_proba(test_x)\n",
    "\n",
    "print(\"Loss function: \", model.loss)\n",
    "#print(\"Loss value: \", model.loss_)\n",
    "print(\"Score: \", model.score(test_x, test_y)) # score = mean accuracy on given data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tolerance = 0.2\n",
    "\n",
    "def round_prediction(y):\n",
    "    result = []\n",
    "    for i in range(len(y)):\n",
    "        if (y[i] - tolerance <= 0):\n",
    "            result.append(0)\n",
    "        elif (y[i] + tolerance >= 1):\n",
    "            result.append(1)\n",
    "        else:\n",
    "            return np.zeros(len(y))\n",
    "    return result\n",
    "    \n",
    "rounded_predictions = np.copy(predictions)\n",
    "\n",
    "# Round predictions based on tolerance:\n",
    "for i in range(len(predictions)):\n",
    "    rounded_predictions[i] = round_prediction(predictions[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " WORD  ->  PREDICTED (first 20 words)\n",
      "=====================================\n",
      "  est  ->  es    ( [0 0 0 0] [0.001, 0.0, 0.967, 0.014] )\n",
      "   ad  ->  et    ( [0 0 0 0] [0.0, 0.002, 0.002, 0.955] )\n",
      "   ac  ->  es    ( [0 0 0 0] [0.001, 0.0, 0.975, 0.006] )\n",
      "   ad  ->  et    ( [0 0 0 0] [0.0, 0.002, 0.002, 0.955] )\n",
      "   ea  ->  es    ( [0 0 0 0] [0.0, 0.0, 0.959, 0.009] )\n",
      "   ac  ->  es    ( [0 0 0 0] [0.001, 0.0, 0.975, 0.006] )\n",
      "   ac  ->  es    ( [0 0 0 0] [0.001, 0.0, 0.975, 0.006] )\n",
      "   at  ->  et    ( [0 0 0 0] [0.0, 0.002, 0.004, 0.976] )\n",
      "  est  ->  es    ( [0 0 0 0] [0.001, 0.0, 0.967, 0.014] )\n",
      "  est  ->  es    ( [0 0 0 0] [0.001, 0.0, 0.967, 0.014] )\n",
      "   at  ->  et    ( [0 0 0 0] [0.0, 0.002, 0.004, 0.976] )\n",
      "  est  ->  es    ( [0 0 0 0] [0.001, 0.0, 0.967, 0.014] )\n",
      "   at  ->  et    ( [0 0 0 0] [0.0, 0.002, 0.004, 0.976] )\n",
      "   at  ->  et    ( [0 0 0 0] [0.0, 0.002, 0.004, 0.976] )\n",
      "   ab  ->  et    ( [0 0 0 0] [0.0, 0.003, 0.001, 0.928] )\n",
      "   at  ->  et    ( [0 0 0 0] [0.0, 0.002, 0.004, 0.976] )\n",
      "   ad  ->  et    ( [0 0 0 0] [0.0, 0.002, 0.002, 0.955] )\n",
      "  est  ->  es    ( [0 0 0 0] [0.001, 0.0, 0.967, 0.014] )\n",
      "  est  ->  es    ( [0 0 0 0] [0.001, 0.0, 0.967, 0.014] )\n",
      "  est  ->  es    ( [0 0 0 0] [0.001, 0.0, 0.967, 0.014] )\n"
     ]
    }
   ],
   "source": [
    "print(\" WORD  ->  PREDICTED (first 20 words)\")\n",
    "print(\"=====================================\")\n",
    "\n",
    "mistakes = []\n",
    "\n",
    "printed = 0\n",
    "for i in range(len(test_x)):\n",
    "    if not np.array_equal(rounded_predictions[i], test_y[i]):\n",
    "        for j in range(len(table)):\n",
    "            predicted_word = \"\"\n",
    "            if (np.array_equal(table[j][1], rounded_predictions[i])):\n",
    "                predicted_word = table[j][2]\n",
    "                break\n",
    "        #print(test_y[i], predictions[i])\n",
    "        print(test_z[i].rjust(MAX_WORD_LENGHT), \" -> \", predicted_word, \"   (\", test_y[i], [float(\"%.3f\" % value) for value in predictions[i]], \")\")\n",
    "        printed += 1\n",
    "    if (printed >= 20):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x29fc6ce8da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(model.loss_curve_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n"
     ]
    }
   ],
   "source": [
    "print(\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
