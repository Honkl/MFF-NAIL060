{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagtion implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-x)) # logsig\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)*(1.0-sigmoid(x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1.0 - x**2\n",
    "\n",
    "def lin(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, layers, activation='tanh'):\n",
    "        if activation == 'sigmoid':\n",
    "            self.activation = sigmoid\n",
    "            self.activation_prime = sigmoid_prime\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = tanh\n",
    "            self.activation_prime = tanh_prime\n",
    "\n",
    "        # Set weights\n",
    "        self.weights = []\n",
    "        # layers = [2,2,1]\n",
    "        # range of weight values (-1,1)\n",
    "        # input and hidden layers - random((2+1, 2+1)) : 3 x 3\n",
    "        for i in range(1, len(layers) - 1):\n",
    "            r = 2*np.random.random((layers[i-1] + 1, layers[i] + 1)) -1\n",
    "            self.weights.append(r)\n",
    "        # output layer - random((2+1, 1)) : 3 x 1\n",
    "        r = 2*np.random.random( (layers[i] + 1, layers[i+1])) - 1\n",
    "        self.weights.append(r)\n",
    "\n",
    "    # fits the network using backpropagation algorthm\n",
    "    def fit(self, X, y, learning_rate=0.2, epochs=100000):\n",
    "        # Add column of ones to X\n",
    "        # This is to add the bias unit to the input layer\n",
    "        ones = np.atleast_2d(np.ones(X.shape[0]))\n",
    "        X = np.concatenate((ones.T, X), axis=1)\n",
    "         \n",
    "        for k in range(epochs):\n",
    "            #if k % 10000 == 0: print('epochs:', k)\n",
    "            \n",
    "            i = np.random.randint(X.shape[0])\n",
    "            a = [X[i]]\n",
    "\n",
    "            for l in range(len(self.weights)):\n",
    "                    dot_value = np.dot(a[l], self.weights[l])\n",
    "                    activation = self.activation(dot_value)\n",
    "                    a.append(activation)\n",
    "            # output layer\n",
    "            error = y[i] - a[-1]\n",
    "            deltas = [error * self.activation_prime(a[-1])]\n",
    "\n",
    "            # we need to begin at the second to last layer \n",
    "            # (a layer before the output layer)\n",
    "            for l in range(len(a) - 2, 0, -1): \n",
    "                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_prime(a[l]))\n",
    "\n",
    "            # reverse\n",
    "            # [level3(output)->level2(hidden)]  => [level2(hidden)->level3(output)]\n",
    "            deltas.reverse()\n",
    "\n",
    "            # backpropagation\n",
    "            # 1. Multiply its output delta and input activation \n",
    "            #    to get the gradient of the weight.\n",
    "            # 2. Subtract a ratio (percentage) of the gradient from the weight.\n",
    "            for i in range(len(self.weights)):\n",
    "                layer = np.atleast_2d(a[i])\n",
    "                delta = np.atleast_2d(deltas[i])\n",
    "                self.weights[i] += learning_rate * layer.T.dot(delta)\n",
    "        \n",
    "        return self.weights\n",
    "\n",
    "    def predict(self, x): \n",
    "        a = np.concatenate((np.ones(1).T, np.array(x)), axis=0)      \n",
    "        for l in range(0, len(self.weights)):\n",
    "            a = self.activation(np.dot(a, self.weights[l]))\n",
    "        return a\n",
    "    \n",
    "#     def predict(self, x):\n",
    "#         x = np.array(x)\n",
    "#         temp = np.ones(x.shape[0]+1)\n",
    "#         temp[0:-1] = x\n",
    "#         a = temp\n",
    "#         for l in range(0, len(self.weights)):\n",
    "#             a = self.activation(np.dot(a, self.weights[l]))\n",
    "#         return a\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants + data generation\n",
    " Constants used in computations, read files with train + test data. Generation of the rest train data. Word transformations (\"word\" => bin)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some basic constants...\n",
    "LETTERS = 25\n",
    "MAX_WORD_LENGHT = 5\n",
    "CHAR_BITS = 5 # Enough for 25 letters\n",
    "#input_size = MAX_WORD_LENGHT * (len(bin(LETTERS))-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_word(word):\n",
    "    word = word.lower()\n",
    "    bin_word = ''.join([bin(ord(x)-ord('a'))[2:].zfill(CHAR_BITS) for x in word])\n",
    "    bin_word = bin_word + \"1\" * CHAR_BITS * (MAX_WORD_LENGHT - len(word))\n",
    "    return list(map(int, bin_word))\n",
    "\n",
    "def encode_unary(index, total_words):\n",
    "    zeros = [0] * total_words\n",
    "    zeros[index] = 1\n",
    "    return zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0], [1, 0, 0, 0], 'lorem'), ([0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0], [0, 1, 0, 0], 'ipsum'), ([0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 1, 0], 'es'), ([0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 1], 'et')]\n"
     ]
    }
   ],
   "source": [
    "f = \"table.txt\"\n",
    "with open (f, \"r\") as file:\n",
    "    raw_data = file.read().strip().split()\n",
    "\n",
    "data = []\n",
    "for word in raw_data:\n",
    "    if (len(word) <= MAX_WORD_LENGHT and word not in data):\n",
    "        data.append(word.lower())\n",
    "\n",
    "table = [(transform_word(data[index]), encode_unary(index, len(data)), data[index]) for index in range(len(data))]\n",
    "output_size = len(table)\n",
    "print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size = MAX_WORD_LENGHT * CHAR_BITS\n",
    "hidden_sizes = [10]\n",
    "layers = [input_size] + hidden_sizes + [output_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_words(set_size, good_word_prob):\n",
    "    X = np.array([table[0][0]])\n",
    "    y = np.array([table[0][1]])\n",
    "    for _ in range(set_size - 1):\n",
    "        if np.random.uniform(0, 1) < good_word_prob:\n",
    "            index = np.random.randint(len(table))\n",
    "            X = np.concatenate((X, [table[index][0]]))\n",
    "            y = np.concatenate((y, [table[index][1]]))\n",
    "        else:\n",
    "            found = False\n",
    "            while not found:\n",
    "                found = True\n",
    "                bad_word = [np.random.randint(2) for _ in range(input_size)]\n",
    "                for i in range(len(table)):\n",
    "                    if (table[i][0] == bad_word):\n",
    "                        found = False\n",
    "                        break\n",
    "                if found:\n",
    "                    X = np.concatenate((X, [bad_word]))\n",
    "                    y = np.concatenate((y, [[0] * output_size]))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x, train_y = generate_words(set_size=100000, good_word_prob=0.6)\n",
    "#test_x, test_y = generate_words(set_size=1000, good_word_prob=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = \"data.txt\"\n",
    "with open (f, \"r\") as file:\n",
    "    raw_data = file.read().strip().split()\n",
    "\n",
    "escape_chars = [\".\", \",\", \"!\", \"?\", \";\"]\n",
    "    \n",
    "data = []\n",
    "for word in raw_data:\n",
    "    if (len(word) <= MAX_WORD_LENGHT):\n",
    "        w = word\n",
    "        for i in range(len(escape_chars)):\n",
    "            w = w.replace(escape_chars[i], \"\")\n",
    "\n",
    "        if (w.isalpha):\n",
    "            data.append(w.lower())\n",
    "        \n",
    "\n",
    "test_x = np.array([transform_word(w) for w in data])\n",
    "test_y = np.array([[0] * len(table) for _ in range(len(test_x))])\n",
    "test_z = np.array(data)\n",
    "\n",
    "for i in range(len(test_x)):\n",
    "    for j in range(len(table)):\n",
    "        if (np.array_equal(table[j][0], test_x[i])):\n",
    "            test_y[i] = table[j][1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning v1\n",
    "Using backprop \"hand\" implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit complete\n"
     ]
    }
   ],
   "source": [
    "net = NeuralNetwork(layers, activation='tanh')\n",
    "_ = net.fit(train_x, train_y, epochs=100000)\n",
    "print(\"Fit complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0] [1 0 0 0] [0.993, 0.001, 0.083, 0.058]\n",
      "[0 1 0 0 0 0 1 1 1 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0] [0 1 0 0] [-0.0, 0.994, -0.016, 0.006]\n",
      "[0 0 0 1 1 0 1 1 1 0 0 1 0 1 1 0 1 1 1 0 1 0 0 0 1] [0 0 0 0] [0.001, 0.001, 0.092, 0.017]\n",
      "[1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1] [0 0 0 0] [0.001, 0.001, 0.058, 0.003]\n",
      "[0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 1 1 1] [0 0 0 0] [0.001, 0.002, 0.043, -0.002]\n",
      "[0 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 1] [0 0 0 0] [0.001, 0.001, 0.024, -0.005]\n",
      "[1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1] [0 0 0 0] [0.001, 0.001, 0.062, 0.013]\n",
      "[1 0 1 0 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1] [0 0 0 0] [0.001, 0.001, 0.062, 0.015]\n",
      "[0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 0] [0 0 0 0] [0.009, 0.002, 0.079, 0.014]\n",
      "[0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 0 1 1 1 0 1 1 1 1 1] [0 0 0 0] [-0.0, -0.001, -0.024, -0.017]\n",
      "Succ:  0.981001016216\n",
      "Err:   0.018998983784\n"
     ]
    }
   ],
   "source": [
    "def pretty_print(x, y, predicted_y):\n",
    "    print(x, y, [float(\"%.3f\" % value) for value in predicted_y])\n",
    "\n",
    "# success of the net using MSE\n",
    "def success(test_x, test_y):\n",
    "    MSE = 0\n",
    "    for (x, y) in zip(test_x, test_y):\n",
    "        prediction = net.predict(x)\n",
    "        MSE += sum((prediction - y) ** 2)\n",
    "    return 1 - (MSE / (len(test_x) * output_size))\n",
    "    \n",
    "show_max = 10\n",
    "count = 0\n",
    "for (x, y) in zip(test_x, test_y):\n",
    "    pretty_print(x, y, net.predict(x))\n",
    "    count += 1\n",
    "    if (show_max == count):\n",
    "        break\n",
    "    \n",
    "\n",
    "s = success(test_x, test_y)\n",
    "print(\"Succ: \", s)\n",
    "print(\"Err:  \", 1 - s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning v2\n",
    "Using scikit-learn MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.39966525\n",
      "Iteration 2, loss = 0.40192144\n",
      "Iteration 3, loss = 0.18980299\n",
      "Iteration 4, loss = 0.09744146\n",
      "Iteration 5, loss = 0.05967717\n",
      "Iteration 6, loss = 0.04067186\n",
      "Iteration 7, loss = 0.02946414\n",
      "Iteration 8, loss = 0.02215357\n",
      "Iteration 9, loss = 0.01705736\n",
      "Iteration 10, loss = 0.01327547\n",
      "Iteration 11, loss = 0.01049011\n",
      "Iteration 12, loss = 0.00838742\n",
      "Iteration 13, loss = 0.00677004\n",
      "Iteration 14, loss = 0.00552648\n",
      "Iteration 15, loss = 0.00451623\n",
      "Iteration 16, loss = 0.00373735\n",
      "Iteration 17, loss = 0.00312631\n",
      "Iteration 18, loss = 0.00260713\n",
      "Iteration 19, loss = 0.00219170\n",
      "Iteration 20, loss = 0.00186373\n",
      "Iteration 21, loss = 0.00156728\n",
      "Iteration 22, loss = 0.00134896\n",
      "Iteration 23, loss = 0.00118184\n",
      "Iteration 24, loss = 0.00101149\n",
      "Iteration 25, loss = 0.00086740\n",
      "Iteration 26, loss = 0.00076047\n",
      "Iteration 27, loss = 0.00066425\n",
      "Iteration 28, loss = 0.00057812\n",
      "Iteration 29, loss = 0.00051547\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Loss function:  log_loss\n",
      "Score:  0.937717987045\n"
     ]
    }
   ],
   "source": [
    "from sklearn import neural_network as nn\n",
    "clf = nn.MLPClassifier(hidden_layer_sizes=(8, ),\n",
    "                       activation='relu',\n",
    "                       solver='adam',\n",
    "                       alpha=0.00001,\n",
    "                       verbose=True,\n",
    "                       tol=0.0001,\n",
    "                       learning_rate='constant',\n",
    "                       learning_rate_init=0.001)\n",
    "\n",
    "model = clf.fit(train_x, train_y)\n",
    "predictions = model.predict_proba(test_x)\n",
    "\n",
    "print(\"Loss function: \", model.loss)\n",
    "#print(\"Loss value: \", model.loss_)\n",
    "print(\"Score: \", model.score(test_x, test_y)) # score = mean accuracy on given data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tolerance = 0.2\n",
    "\n",
    "def round_prediction(y):\n",
    "    result = []\n",
    "    for i in range(len(y)):\n",
    "        if (y[i] - tolerance <= 0):\n",
    "            result.append(0)\n",
    "        elif (y[i] + tolerance >= 1):\n",
    "            result.append(1)\n",
    "        else:\n",
    "            return np.zeros(len(y))\n",
    "    return result\n",
    "    \n",
    "rounded_predictions = np.copy(predictions)\n",
    "\n",
    "# Round predictions based on tolerance:\n",
    "for i in range(len(predictions)):\n",
    "    rounded_predictions[i] = round_prediction(predictions[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test for table (positive words): \n",
      "Score on table words:  1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Test for table (positive words): \")\n",
    "\n",
    "\n",
    "table_test_x = []\n",
    "table_test_y = []\n",
    "for i in range(len(table)):\n",
    "    table_test_x.append(table[i][0])\n",
    "    table_test_y.append(table[i][1])\n",
    "    \n",
    "table_test_x = np.array(table_test_x)\n",
    "table_test_y = np.array(table_test_y)\n",
    "\n",
    "\n",
    "print(\"Score on table words: \", model.score(table_test_x, table_test_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " WORD  ->  PREDICTED (first 20 words)\n",
      "=====================================\n",
      "  est  ->  es    [0 0 0 0]  ->  [0.0, 0.0, 0.98, 0.003]\n",
      "   ad  ->  et    [0 0 0 0]  ->  [0.0, 0.0, 0.0, 0.985]\n",
      "   ac  ->  es    [0 0 0 0]  ->  [0.0, 0.0, 0.977, 0.0]\n",
      "   ad  ->  et    [0 0 0 0]  ->  [0.0, 0.0, 0.0, 0.985]\n",
      "   ea  ->  es    [0 0 0 0]  ->  [0.0, 0.0, 0.959, 0.0]\n",
      "   ac  ->  es    [0 0 0 0]  ->  [0.0, 0.0, 0.977, 0.0]\n",
      "   ac  ->  es    [0 0 0 0]  ->  [0.0, 0.0, 0.977, 0.0]\n",
      "   at  ->  et    [0 0 0 0]  ->  [0.0, 0.0, 0.0, 0.995]\n",
      "  est  ->  es    [0 0 0 0]  ->  [0.0, 0.0, 0.98, 0.003]\n",
      "  est  ->  es    [0 0 0 0]  ->  [0.0, 0.0, 0.98, 0.003]\n",
      "   at  ->  et    [0 0 0 0]  ->  [0.0, 0.0, 0.0, 0.995]\n",
      "  est  ->  es    [0 0 0 0]  ->  [0.0, 0.0, 0.98, 0.003]\n",
      "   at  ->  et    [0 0 0 0]  ->  [0.0, 0.0, 0.0, 0.995]\n",
      "   at  ->  et    [0 0 0 0]  ->  [0.0, 0.0, 0.0, 0.995]\n",
      "   ab  ->  et    [0 0 0 0]  ->  [0.0, 0.0, 0.0, 0.892]\n",
      "   at  ->  et    [0 0 0 0]  ->  [0.0, 0.0, 0.0, 0.995]\n",
      "   ad  ->  et    [0 0 0 0]  ->  [0.0, 0.0, 0.0, 0.985]\n",
      "  est  ->  es    [0 0 0 0]  ->  [0.0, 0.0, 0.98, 0.003]\n",
      "  est  ->  es    [0 0 0 0]  ->  [0.0, 0.0, 0.98, 0.003]\n",
      "  est  ->  es    [0 0 0 0]  ->  [0.0, 0.0, 0.98, 0.003]\n"
     ]
    }
   ],
   "source": [
    "print(\" WORD  ->  PREDICTED (first 20 words)\")\n",
    "print(\"=====================================\")\n",
    "\n",
    "mistakes = []\n",
    "\n",
    "printed = 0\n",
    "for i in range(len(test_x)):\n",
    "    if not np.array_equal(rounded_predictions[i], test_y[i]):\n",
    "        for j in range(len(table)):\n",
    "            predicted_word = \"\"\n",
    "            if (np.array_equal(table[j][1], rounded_predictions[i])):\n",
    "                predicted_word = table[j][2]\n",
    "                break\n",
    "        #print(test_y[i], predictions[i])\n",
    "        print(test_z[i].rjust(MAX_WORD_LENGHT), \" -> \", predicted_word, \"  \", test_y[i], \" -> \", [float(\"%.3f\" % value) for value in predictions[i]])\n",
    "        printed += 1\n",
    "    if (printed >= 20):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2201c826ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(model.loss_curve_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
